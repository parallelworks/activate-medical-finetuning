# yaml-language-server: $schema=https://activate.parallel.works/workflow.schema.json

# ==============================================================================
# Medical LLM Fine-tuning Workflow
# ==============================================================================
# Fine-tunes LLMs (Llama2/3, Mistral, Gemma) on medical datasets using QLoRA/LoRA
# via Singularity containers. Generates vLLM-compatible merged weights.
#
# Organized Input Sections:
#   1. Infrastructure & Compute    - GPU cluster, containers, storage
#   2. Model Selection             - Base model and configuration presets
#   3. Dataset Configuration       - Training data and sampling
#   4. Training Hyperparameters    - Optimizer and batch settings
#   5. LoRA / PEFT Configuration   - Adapter and quantization settings
#   6. TensorBoard Monitoring      - Training visualization
#   7. Output & Publishing         - Model merging and hub publishing
# ==============================================================================

permissions:
  - '*'

sessions:
  tensorboard:
    useTLS: false
    redirect: true
    useCustomDomain: false

on:
  execute:
    inputs:
      cluster:
        label: GPU Cluster
        type: compute-clusters
        optional: false

      infrastructure:
        type: group
        label: Infrastructure & Compute
        collapsed: false
        items:
          singularity_image:
            label: Singularity Image Path (Optional)
            type: string
            default: "/home/$USER/pw/singularity/medical-finetune.sif"
            optional: true

          output_bucket:
            label: Output Storage Bucket (Optional - leave empty to skip)
            type: storage
            default: ""
            optional: true

      model_selection:
        type: group
        label: Model Selection
        collapsed: false
        items:
          model_config:
            label: Configuration Preset
            type: dropdown
            default: llama3-8b-medical-quick
            optional: true
            options:
              - value: llama3-8b-medical-quick
                label: Llama 3 8B Medical (Quick Test)
              - value: llama3-8b-medical
                label: Llama 3 8B Medical (Production)
              - value: mistral-7b-medical
                label: Mistral 7B Medical
              - value: gemma-7b-medical
                label: Gemma 7B Medical

          base_model_id:
            label: Base Model ID (HuggingFace)
            type: string
            default: "${{ inputs.model_selection.model_config == 'gemma-7b-medical' ? 'google/gemma-1.1-7b-it' : (inputs.model_selection.model_config == 'mistral-7b-medical' ? 'mistralai/Mistral-7B-Instruct-v0.2' : 'meta-llama/Llama-3.1-8B-Instruct') }}"
            optional: true

      dataset_config:
        type: group
        label: Dataset Configuration
        collapsed: true
        items:
          dataset_source:
            label: Dataset Source
            type: dropdown
            default: huggingface
            options:
              - value: huggingface
                label: HuggingFace Hub
              - value: local
                label: Local File Path
              - value: bucket
                label: PW Storage Bucket

          dataset_cache_dir:
            label: Dataset Cache Directory
            type: string
            default: "/home/$USER/pw/datasets"
            optional: true
            description: Shared location for caching HuggingFace datasets across runs

          dataset_name:
            label: Dataset Name (HuggingFace)
            type: string
            default: "${{ inputs.model_selection.model_config == 'llama3-8b-medical-quick' ? 'Shekswess/medical_llama3_instruct_dataset_short' : (inputs.model_selection.model_config == 'mistral-7b-medical' ? 'Shekswess/medical_mistral_instruct_dataset' : (inputs.model_selection.model_config == 'gemma-7b-medical' ? 'Shekswess/medical_gemma_instruct_dataset' : 'Shekswess/medical_llama3_instruct_dataset')) }}"
            hidden: ${{ inputs.dataset_config.dataset_source != 'huggingface' }}

          dataset_bucket:
            label: Dataset Storage Bucket
            type: storage
            default: ""
            optional: true
            hidden: ${{ inputs.dataset_config.dataset_source != 'bucket' }}

          dataset_bucket_path:
            label: Dataset Bucket Path
            type: string
            default: ""
            optional: true
            hidden: ${{ inputs.dataset_config.dataset_source != 'bucket' }}

          local_dataset_path:
            label: Local Dataset Path
            type: string
            default: ""
            optional: true
            hidden: ${{ inputs.dataset_config.dataset_source != 'local' }}

          dataset_split:
            label: Dataset Split
            type: string
            default: train
            optional: true
            hidden: ${{ inputs.dataset_config.dataset_source == 'bucket' }}

          dataset_config_name:
            label: Dataset Config Name
            type: string
            default: ""
            optional: true
            hidden: ${{ inputs.dataset_config.dataset_source != 'huggingface' }}

          dataset_format:
            label: Dataset Format
            type: dropdown
            default: json
            optional: true
            hidden: ${{ inputs.dataset_config.dataset_source != 'local' }}
            options:
              - value: json
                label: JSON / JSONL
              - value: csv
                label: CSV
              - value: parquet
                label: Parquet
              - value: arrow
                label: Arrow
              - value: dataset
                label: HuggingFace Dataset Directory

          prompt_field:
            label: Prompt Field Name
            type: string
            default: prompt
            optional: true

          max_samples:
            label: Max Samples (0 = all)
            type: number
            default: "${{ inputs.model_selection.model_config == 'llama3-8b-medical-quick' ? 50 : 0 }}"
            optional: true

      training_hyperparameters:
        type: group
        label: Training Hyperparameters
        collapsed: true
        items:
          num_epochs:
            label: Training Epochs
            type: number
            default: "${{ inputs.model_selection.model_config == 'llama3-8b-medical-quick' ? 1.0 : 3.0 }}"
            optional: true

          learning_rate:
            label: Learning Rate
            type: number
            default: 0.0002
            optional: true

          micro_batch_size:
            label: Micro Batch Size (per GPU)
            type: number
            default: "${{ inputs.model_selection.model_config == 'llama3-8b-medical-quick' ? 2 : 1 }}"
            optional: true

          gradient_accumulation:
            label: Gradient Accumulation Steps
            type: number
            default: "${{ inputs.model_selection.model_config == 'llama3-8b-medical-quick' ? 8 : 16 }}"
            optional: true

          max_seq_length:
            label: Max Sequence Length (tokens)
            type: number
            default: "${{ inputs.model_selection.model_config == 'llama3-8b-medical-quick' ? 512 : 2048 }}"
            optional: true

      lora_config:
        type: group
        label: LoRA / PEFT Configuration
        collapsed: true
        items:
          lora_r:
            label: LoRA Rank (r)
            type: number
            default: "${{ inputs.model_selection.model_config == 'llama3-8b-medical-quick' ? 32 : 64 }}"
            optional: true

          lora_alpha:
            label: LoRA Alpha
            type: number
            default: 16
            optional: true

          lora_dropout:
            label: LoRA Dropout Rate
            type: number
            default: 0.05
            optional: true

          quantization:
            label: Quantization Mode
            type: dropdown
            default: 4bit
            optional: true
            options:
              - value: 4bit
              - value: 8bit
              - value: none

          gradient_checkpointing:
            label: Enable Gradient Checkpointing
            type: boolean
            default: true
            optional: true

          bf16:
            label: Use BF16 Mixed Precision
            type: boolean
            default: true
            optional: true

          packing:
            label: Enable Dataset Packing
            type: boolean
            default: false
            optional: true

      tensorboard_config:
        type: group
        label: TensorBoard Monitoring
        collapsed: false
        items:
          tensorboard_enabled:
            label: Enable TensorBoard
            type: boolean
            default: true
            optional: true

          tensorboard_port:
            label: TensorBoard Port
            type: number
            default: 6006
            optional: true
            hidden: ${{ !inputs.tensorboard_config.tensorboard_enabled }}

      output_publishing:
        type: group
        label: Output & Publishing
        collapsed: true
        items:
          merge_full_weights:
            label: Merge LoRA into Full Weights
            type: boolean
            default: true
            optional: true

          merged_save_format:
            label: Merged Weights Format
            type: dropdown
            default: safetensors
            optional: true
            options:
              - value: safetensors
              - value: bin

          push_to_hub:
            label: Push to HuggingFace Hub
            type: boolean
            default: false
            optional: true

          hub_model_id:
            label: Hub Model ID (username/model-name)
            type: string
            default: ""
            optional: true

          hf_token:
            label: HuggingFace Access Token
            type: password
            default: ${{ org.HF_TOKEN }}
            optional: true


jobs:
  setup:
    steps:
      - name: Verify GPU availability
        run: |
          echo "Checking GPU availability on ${{ inputs.cluster.name }}..."
          nvidia-smi || { echo "No GPU available"; exit 1; }
          echo "GPU(s) found:"
          nvidia-smi --query-gpu=name,memory.total --format=csv,noheader
        ssh:
          remoteHost: ${{ inputs.cluster.ip }}

      - name: Clone source repository
        run: |
          if [ ! -d "LLM-Medical-Finetuning" ]; then
            git clone https://github.com/Shekswess/LLM-Medical-Finetuning.git
          fi
          cd LLM-Medical-Finetuning
          git log -1 --oneline
        ssh:
          remoteHost: ${{ inputs.cluster.ip }}

  prepare-dataset:
    needs: [setup]
    steps:
      - name: Prepare dataset based on source
        run: |
          OUTPUT_DIR="${HOME}/pw/outputs/medical-finetune-${PW_RUN_ID}"
          DATASET_DIR="${OUTPUT_DIR}/dataset"
          DATASET_CACHE_DIR="${{ inputs.dataset_config.dataset_cache_dir }}"
          mkdir -p "${DATASET_DIR}" "${DATASET_CACHE_DIR}"

          DATASET_SOURCE="${{ inputs.dataset_config.dataset_source }}"
          DATASET_NAME="${{ inputs.dataset_config.dataset_name }}"
          echo "Dataset source: ${DATASET_SOURCE}"

          if [ "${DATASET_SOURCE}" == "bucket" ]; then
            echo "Downloading dataset from PW Storage Bucket..."
            BUCKET="${{ inputs.dataset_config.dataset_bucket }}"
            BUCKET_PATH="${{ inputs.dataset_config.dataset_bucket_path }}"

            if [ -z "${BUCKET}" ] || [ -z "${BUCKET_PATH}" ]; then
              echo "Error: bucket and bucket_path must be specified for bucket source"
              exit 1
            fi

            pw storage cp -r "${BUCKET}/${BUCKET_PATH}" "${DATASET_DIR}/"
            echo "Dataset downloaded to ${DATASET_DIR}"
            ls -la "${DATASET_DIR}"

          elif [ "${DATASET_SOURCE}" == "local" ]; then
            echo "Using local dataset path..."
            LOCAL_PATH="${{ inputs.dataset_config.local_dataset_path }}"

            if [ ! -e "${LOCAL_PATH}" ]; then
              echo "Error: Local dataset path not found: ${LOCAL_PATH}"
              exit 1
            fi

            if [ -f "${LOCAL_PATH}" ]; then
              cp "${LOCAL_PATH}" "${DATASET_DIR}/"
            elif [ -d "${LOCAL_PATH}" ]; then
              cp -r "${LOCAL_PATH}"/* "${DATASET_DIR}/"
            fi
            echo "Dataset copied to ${DATASET_DIR}"
            ls -la "${DATASET_DIR}"

          else
            echo "Using HuggingFace Hub dataset: ${DATASET_NAME}"
            echo "Cache directory: ${DATASET_CACHE_DIR}"
            
            # Check if dataset is already cached
            # HuggingFace datasets library will use HF_DATASETS_CACHE
            export HF_DATASETS_CACHE="${DATASET_CACHE_DIR}"
            
            # Create a marker file to track cached datasets
            CACHE_MARKER="${DATASET_CACHE_DIR}/.cached_datasets"
            if grep -q "^${DATASET_NAME}$" "${CACHE_MARKER}" 2>/dev/null; then
              echo "Dataset '${DATASET_NAME}' found in cache!"
            else
              echo "Dataset will be downloaded on first use and cached for future runs"
            fi
          fi

          echo "DATASET_DIR=${DATASET_DIR}" >> $GITHUB_ENV
          echo "HF_DATASETS_CACHE=${DATASET_CACHE_DIR}" >> $GITHUB_ENV
        ssh:
          remoteHost: ${{ inputs.cluster.ip }}

  build-container:
    needs: [prepare-dataset]
    steps:
      - name: Build Singularity container
        run: |
          CONTAINER_DIR="${HOME}/pw/singularity"
          mkdir -p "${CONTAINER_DIR}"
          cd "${CONTAINER_DIR}"

          if [ -n "${{ inputs.infrastructure.singularity_image }}" ]; then
            echo "Using provided Singularity image: ${{ inputs.infrastructure.singularity_image }}"
            if [ ! -f "${{ inputs.infrastructure.singularity_image }}" ]; then
              echo "Error: Image not found at ${{ inputs.infrastructure.singularity_image }}"
              exit 1
            fi
            export SIF_PATH="${{ inputs.infrastructure.singularity_image }}"
          else
            DEF_FILE="${HOME}/pw/activate-medical-finetuning/singularity/finetune.def"
            SIF_PATH="${CONTAINER_DIR}/medical-finetune.sif"

            if [ ! -f "${SIF_PATH}" ]; then
              echo "Building Singularity image from ${DEF_FILE}..."
              sudo singularity build "${SIF_PATH}" "${DEF_FILE}" || \
                singularity build "${SIF_PATH}" "${DEF_FILE}"
            else
              echo "Using existing Singularity image: ${SIF_PATH}"
            fi
          fi

          echo "SIF_PATH=${SIF_PATH}" >> $GITHUB_ENV
        ssh:
          remoteHost: ${{ inputs.cluster.ip }}
        env:
          SINGULARITY_DISABLE_CACHE: "false"

  finetune:
    needs: [build-container, prepare-dataset]
    timeout: 24h
    steps:
      - name: Execute fine-tuning
        run: |
          OUTPUT_DIR="${HOME}/pw/outputs/medical-finetune-${PW_RUN_ID}"
          WORKDIR="${HOME}/pw/activate-medical-finetuning"
          
          # Determine SIF path
          if [ -n "${{ inputs.infrastructure.singularity_image }}" ] && [ -f "${{ inputs.infrastructure.singularity_image }}" ]; then
            export SIF_PATH="${{ inputs.infrastructure.singularity_image }}"
          elif [ -f "${HOME}/pw/singularity/medical-finetune.sif" ]; then
            export SIF_PATH="${HOME}/pw/singularity/medical-finetune.sif"
          else
            echo "Error: Singularity image not found"
            exit 1
          fi
          
          # Export all training configuration as environment variables
          export OUTPUT_DIR="${OUTPUT_DIR}"
          export MODEL_ID="${{ inputs.model_selection.base_model_id }}"
          export DATASET_SOURCE="${{ inputs.dataset_config.dataset_source }}"
          export DATASET_NAME="${{ inputs.dataset_config.dataset_name }}"
          export DATASET_SPLIT="${{ inputs.dataset_config.dataset_split }}"
          export DATASET_CONFIG_NAME="${{ inputs.dataset_config.dataset_config_name }}"
          export PROMPT_FIELD="${{ inputs.dataset_config.prompt_field }}"
          export LOCAL_DATASET_PATH="${{ inputs.dataset_config.local_dataset_path }}"
          export DATASET_FORMAT="${{ inputs.dataset_config.dataset_format }}"
          export MAX_SAMPLES="${{ inputs.dataset_config.max_samples }}"
          export DATASET_CACHE_DIR="${{ inputs.dataset_config.dataset_cache_dir }}"
          export HF_DATASETS_CACHE="${{ inputs.dataset_config.dataset_cache_dir }}"
          
          # Training hyperparameters
          export NUM_EPOCHS="${{ inputs.training_hyperparameters.num_epochs }}"
          export LEARNING_RATE="${{ inputs.training_hyperparameters.learning_rate }}"
          export MICRO_BATCH_SIZE="${{ inputs.training_hyperparameters.micro_batch_size }}"
          export GRADIENT_ACCUMULATION="${{ inputs.training_hyperparameters.gradient_accumulation }}"
          export MAX_SEQ_LENGTH="${{ inputs.training_hyperparameters.max_seq_length }}"
          
          # LoRA configuration
          export LORA_R="${{ inputs.lora_config.lora_r }}"
          export LORA_ALPHA="${{ inputs.lora_config.lora_alpha }}"
          export LORA_DROPOUT="${{ inputs.lora_config.lora_dropout }}"
          export QUANTIZATION="${{ inputs.lora_config.quantization }}"
          export BF16="${{ inputs.lora_config.bf16 }}"
          export GRADIENT_CHECKPOINTING="${{ inputs.lora_config.gradient_checkpointing }}"
          export PACKING="${{ inputs.lora_config.packing }}"
          
          # Output configuration
          export MERGE_FULL_WEIGHTS="${{ inputs.output_publishing.merge_full_weights }}"
          export MERGED_SAVE_FORMAT="${{ inputs.output_publishing.merged_save_format }}"
          
          # TensorBoard configuration (auto-configured for PW session proxy)
          export TENSORBOARD_ENABLED="${{ inputs.tensorboard_config.tensorboard_enabled }}"
          export TENSORBOARD_PORT="${{ inputs.tensorboard_config.tensorboard_port }}"
          export TENSORBOARD_PATH_PREFIX="/me/session/${PW_USER}/${{ sessions.tensorboard }}"
          export TENSORBOARD_BIND_ALL="true"
          
          # HuggingFace Hub configuration
          if [ -n "${{ inputs.output_publishing.hf_token }}" ]; then
            export HF_TOKEN="${{ inputs.output_publishing.hf_token }}"
          fi
          
          # Handle bucket dataset source
          if [ "${{ inputs.dataset_config.dataset_source }}" == "bucket" ]; then
            export DATASET_DIR="${OUTPUT_DIR}/dataset"
          fi
          
          echo "======================================"
          echo "Training Configuration"
          echo "======================================"
          echo "Model: ${MODEL_ID}"
          echo "Dataset Source: ${DATASET_SOURCE}"
          echo "Dataset: ${DATASET_NAME:-${LOCAL_DATASET_PATH:-bucket}}"
          echo "Epochs: ${NUM_EPOCHS}"
          echo "Max Samples: ${MAX_SAMPLES}"
          echo "TensorBoard: ${TENSORBOARD_ENABLED}"
          echo "======================================"
          
          # Execute training using run.sh
          cd "${WORKDIR}"
          ./scripts/run.sh
          
          echo "Training completed. Checking outputs..."
          ls -la "${OUTPUT_DIR}/adapters" 2>/dev/null || echo "No adapters found"
          if [ "${MERGE_FULL_WEIGHTS}" == "true" ]; then
            ls -la "${OUTPUT_DIR}/merged" 2>/dev/null || echo "No merged weights found"
          fi
        ssh:
          remoteHost: ${{ inputs.cluster.ip }}
        timeout: 24h

      - name: Validate outputs
        run: |
          OUTPUT_DIR="${HOME}/pw/outputs/medical-finetune-${PW_RUN_ID}"
          if [ ! -d "${OUTPUT_DIR}/adapters" ]; then
            echo "Error: No adapter outputs found"
            exit 1
          fi

          if [ "${{ inputs.output_publishing.merge_full_weights }}" == "true" ] && [ ! -d "${OUTPUT_DIR}/merged" ]; then
            echo "Error: Merge requested but no merged weights found"
            exit 1
          fi

          echo "Training outputs validated successfully"
          echo "Adapter files:"
          ls -la "${OUTPUT_DIR}/adapters"

          if [ -d "${OUTPUT_DIR}/merged" ]; then
            echo "Merged weight files:"
            ls -la "${OUTPUT_DIR}/merged"
          fi
        ssh:
          remoteHost: ${{ inputs.cluster.ip }}

  tensorboard-session:
    needs: [build-container, prepare-dataset]
    if: ${{ inputs.tensorboard_config.tensorboard_enabled }}
    steps:
      - name: Wait for TensorBoard to start
        run: |
          echo "Waiting for TensorBoard to be available on port ${{ inputs.tensorboard_config.tensorboard_port }}..."
          # Wait up to 5 minutes for TensorBoard to start (finetune job starts it)
          for i in {1..150}; do
            if nc -z localhost ${{ inputs.tensorboard_config.tensorboard_port }} 2>/dev/null; then
              echo "TensorBoard is running on port ${{ inputs.tensorboard_config.tensorboard_port }}!"
              exit 0
            fi
            sleep 2
          done
          echo "Warning: TensorBoard did not start within timeout"
          exit 1
        ssh:
          remoteHost: ${{ inputs.cluster.ip }}
          
      - name: Update TensorBoard Session
        uses: parallelworks/update-session
        with:
          remotePort: ${{ inputs.tensorboard_config.tensorboard_port }}
          name: ${{ sessions.tensorboard }}
          status: running
          target: ${{ inputs.cluster.id }}
          slug: ""
          
      - name: Keep session alive during training
        run: |
          echo "TensorBoard session is active. Waiting for training to complete..."
          # Keep session job running - it will be terminated when workflow completes
          while nc -z localhost ${{ inputs.tensorboard_config.tensorboard_port }} 2>/dev/null; do
            sleep 30
          done
          echo "TensorBoard stopped - training likely completed"
        ssh:
          remoteHost: ${{ inputs.cluster.ip }}

  upload-outputs:
    needs: [finetune]
    if: ${{ inputs.infrastructure.output_bucket }}
    steps:
      - name: Upload weights to bucket
        run: |
          OUTPUT_DIR="${HOME}/pw/outputs/medical-finetune-${PW_RUN_ID}"
          BUCKET_PATH="${{ inputs.infrastructure.output_bucket }}"
          RUN_DIR="medical-finetune-${PW_RUN_ID}"

          echo "Uploading outputs to ${BUCKET_PATH}/${RUN_DIR}..."
          pw storage cp -r "${OUTPUT_DIR}/adapters" "${BUCKET_PATH}/${RUN_DIR}/adapters"

          if [ -d "${OUTPUT_DIR}/merged" ]; then
            pw storage cp -r "${OUTPUT_DIR}/merged" "${BUCKET_PATH}/${RUN_DIR}/merged"
          fi

          if [ -f "${OUTPUT_DIR}/logs/finetune.log" ]; then
            pw storage cp "${OUTPUT_DIR}/logs/finetune.log" "${BUCKET_PATH}/${RUN_DIR}/"
          fi

          echo "Upload complete"
          echo "Files in bucket:"
          pw storage ls "${BUCKET_PATH}/${RUN_DIR}/"
        ssh:
          remoteHost: ${{ inputs.cluster.ip }}

  cleanup:
    needs: [upload-outputs]
    if: ${{ always }}
    steps:
      - name: Display training summary
        run: |
          OUTPUT_DIR="${HOME}/pw/outputs/medical-finetune-${PW_RUN_ID}"
          echo "======================================"
          echo "Medical LLM Fine-tuning Summary"
          echo "======================================"
          echo "Run ID: ${PW_RUN_ID}"
          echo "Model: ${{ inputs.model_selection.base_model_id }}"
          echo "Dataset: ${{ inputs.dataset_config.dataset_name }}"
          echo "Output Directory: ${OUTPUT_DIR}"
          echo ""
          if [ -f "${OUTPUT_DIR}/logs/finetune.log" ]; then
            echo "Last 20 lines of training log:"
            tail -20 "${OUTPUT_DIR}/logs/finetune.log"
          fi
          echo "======================================"
        ssh:
          remoteHost: ${{ inputs.cluster.ip }}

# Note: Configuration presets are now implemented via dynamic defaults in the form inputs.
# When user selects a preset from the dropdown, all related fields automatically update.
# Users can still override any value after selecting a preset.
