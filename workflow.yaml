# yaml-language-server: $schema=https://activate.parallel.works/workflow.schema.json
---
# ==============================================================================
# Medical LLM Fine-tuning Workflow
# ==============================================================================
# Fine-tunes LLMs (Llama2/3, Mistral, Gemma) on medical datasets using QLoRA/LoRA
# via Singularity containers. Generates vLLM-compatible merged weights.
#
# Organized Input Sections:
#   1. Infrastructure & Compute    - GPU cluster, scheduler, containers, storage
#   2. Model Selection             - Base model and configuration presets
#   3. Dataset Configuration       - Training data and sampling
#   4. Training Hyperparameters    - Optimizer and batch settings
#   5. LoRA / PEFT Configuration   - Adapter and quantization settings
#   6. TensorBoard Monitoring      - Training visualization
#   7. Output & Publishing         - Model merging and hub publishing
# ==============================================================================

permissions:
  - '*'

sessions:
  session:
    useTLS: false
    redirect: true
    useCustomDomain: false

on:
  execute:
    inputs:
      cluster:
        label: GPU Cluster
        type: compute-clusters
        optional: false
        autoselect: true
        include-workspace: false

      model_selection:
        type: group
        label: Model Selection
        collapsed: false
        items:
          model_config:
            label: Configuration Preset
            type: dropdown
            default: llama3-8b-medical-quick
            optional: true
            options:
              - value: llama3-8b-medical-quick
                label: Llama 3 8B Medical (Quick Test)
              - value: llama3-8b-medical
                label: Llama 3 8B Medical (Production)
              - value: mistral-7b-medical
                label: Mistral 7B Medical
              - value: gemma-7b-medical
                label: Gemma 7B Medical

          model_source:
            label: Model Source
            type: dropdown
            default: huggingface
            tooltip: Use a pre-downloaded model (local) or clone from HuggingFace using git-lfs
            options:
              - value: local
                label: "Local Path (pre-staged weights)"
              - value: huggingface
                label: "HuggingFace Clone (git-lfs)"

          base_model_id:
            label: HuggingFace Model ID
            type: string
            default: "${{ inputs.model_selection.model_config == 'gemma-7b-medical' ? 'google/gemma-1.1-7b-it' : (inputs.model_selection.model_config == 'mistral-7b-medical' ? 'mistralai/Mistral-7B-Instruct-v0.2' : 'meta-llama/Llama-3.1-8B-Instruct') }}"
            optional: true
            tooltip: Model ID to clone from HuggingFace via git-lfs
            hidden: ${{ inputs.model_selection.model_source == 'local' }}

          local_model_path:
            label: Model Path
            type: string
            default: "/models/Llama-3.1-8B-Instruct"
            optional: true
            hidden: ${{ inputs.model_selection.model_source != 'local' }}
            tooltip: Full path to directory containing model weights

          hf_token:
            label: HuggingFace Token
            optional: true
            default: ${{ org.HF_TOKEN }}
            type: password
            tooltip: Required for gated models (Llama, etc.)
            hidden: ${{ inputs.model_selection.model_source != 'huggingface' }}
            ignore: ${{ inputs.model_selection.model_source != 'huggingface' }}

          model_cache_dir:
            type: string
            label: Model Cache Directory
            default: ~/pw/models
            tooltip: Directory to clone model into. Model is cloned once and reused for subsequent runs. Ensure sufficient disk space.
            hidden: ${{ inputs.model_selection.model_source != 'huggingface' }}
            ignore: ${{ inputs.model_selection.model_source != 'huggingface' }}

      submit_to_scheduler:
        type: boolean
        label: Submit to Job Scheduler
        tooltip: Enable to submit job via SLURM or PBS scheduler (detected automatically from resource). Disable for direct SSH execution.
        default: false
        hidden: ${{ inputs.cluster.schedulerType != 'slurm' && inputs.cluster.schedulerType != 'pbs' }}

      slurm:
        type: group
        label: SLURM Configuration
        hidden: ${{ inputs.submit_to_scheduler != true || inputs.cluster.schedulerType != 'slurm' }}
        collapsed: true
        items:
          is_disabled:
            type: boolean
            default: ${{ inputs.submit_to_scheduler != true || inputs.cluster.schedulerType != 'slurm' }}
            hidden: true

          account:
            label: Account
            type: slurm-accounts
            resource: ${{ inputs.cluster }}
            tooltip: SLURM account for job submission (--account)
            optional: true
            ignore: ${{ inputs.submit_to_scheduler != true || inputs.cluster.schedulerType != 'slurm' }}

          partition:
            type: slurm-partitions
            label: Partition
            optional: true
            resource: ${{ inputs.cluster }}
            tooltip: Partition for job submission (--partition)
            ignore: ${{ inputs.submit_to_scheduler != true || inputs.cluster.schedulerType != 'slurm' }}

          qos:
            label: Quality of Service (QoS)
            type: slurm-qos
            resource: ${{ inputs.cluster }}
            tooltip: SLURM QoS setting (--qos)
            optional: true
            ignore: ${{ inputs.submit_to_scheduler != true || inputs.cluster.schedulerType != 'slurm' }}

          cpus_per_task:
            type: number
            label: CPUs per Task
            min: 1
            max: 256
            default: 1
            tooltip: Number of CPUs for the job (--cpus-per-task)
            ignore: ${{ inputs.submit_to_scheduler != true || inputs.cluster.schedulerType != 'slurm' }}

          time:
            label: Walltime
            type: string
            default: 04:00:00
            tooltip: Maximum job duration, e.g., 04:00:00 (--time)
            ignore: ${{ inputs.submit_to_scheduler != true || inputs.cluster.schedulerType != 'slurm' }}

          gres:
            label: GPUs (gres)
            type: dropdown
            default: "gpu:1"
            tooltip: GPU resource specification (--gres). Keep in sync with training GPU count.
            options:
              - value: "gpu:1"
                label: "1 GPU (gpu:1)"
              - value: "gpu:2"
                label: "2 GPUs (gpu:2)"
              - value: "gpu:4"
                label: "4 GPUs (gpu:4)"
              - value: "gpu:8"
                label: "8 GPUs (gpu:8)"
            ignore: ${{ inputs.submit_to_scheduler != true || inputs.cluster.schedulerType != 'slurm' }}

          scheduler_directives:
            type: editor
            label: Additional Directives
            optional: true
            tooltip: Additional SLURM directives (include #SBATCH prefix)
            ignore: ${{ inputs.submit_to_scheduler != true || inputs.cluster.schedulerType != 'slurm' }}

      pbs:
        type: group
        label: PBS Configuration
        hidden: ${{ inputs.submit_to_scheduler != true || inputs.cluster.schedulerType != 'pbs' }}
        collapsed: true
        items:
          is_disabled:
            type: boolean
            default: ${{ inputs.submit_to_scheduler != true || inputs.cluster.schedulerType != 'pbs' }}
            hidden: true

          account:
            label: Account
            type: string
            optional: true
            tooltip: PBS account for job submission (-A)
            ignore: ${{ inputs.submit_to_scheduler != true || inputs.cluster.schedulerType != 'pbs' }}

          queue:
            label: Queue
            type: string
            optional: true
            tooltip: PBS queue name (-q)
            ignore: ${{ inputs.submit_to_scheduler != true || inputs.cluster.schedulerType != 'pbs' }}

          walltime:
            label: Walltime
            type: string
            default: 04:00:00
            tooltip: Maximum job duration, e.g., 04:00:00 (-l walltime=)
            ignore: ${{ inputs.submit_to_scheduler != true || inputs.cluster.schedulerType != 'pbs' }}

          select:
            label: Resource Selection
            type: string
            default: "1:ncpus=8:ngpus=1"
            placeholder: "1:ncpus=8:ngpus=1"
            tooltip: PBS resource selection string, e.g., 1:ncpus=8:ngpus=1 (-l select=)
            ignore: ${{ inputs.submit_to_scheduler != true || inputs.cluster.schedulerType != 'pbs' }}

          scheduler_directives:
            label: Additional Directives
            type: editor
            tooltip: Additional PBS directives (include #PBS prefix)
            optional: true
            ignore: ${{ inputs.submit_to_scheduler != true || inputs.cluster.schedulerType != 'pbs' }}

      infrastructure:
        type: group
        label: Infrastructure & Compute
        collapsed: true
        items:
          rundir:
            label: Run Directory
            type: string
            default: "~/pw/activate-medical-finetuning"
            optional: false

          repository:
            label: Repository URL
            type: string
            default: "https://github.com/parallelworks/activate-medical-finetuning.git"
            optional: true

          repository_branch:
            label: Repository Branch
            type: string
            default: "main"
            optional: true

      container:
        type: group
        label: Container Options
        collapsed: true
        items:
          source:
            type: dropdown
            label: Container Source
            default: lfs
            tooltip: How to obtain Apptainer/Singularity container
            options:
              - value: lfs
                label: "Git LFS repo (default)"
              - value: path
                label: "Use existing path"
              - value: pull
                label: "Pull from bucket"
              - value: build
                label: "Build from source (requires sudo/fakeroot)"

          lfs_repo:
            label: LFS Repo
            type: string
            default: "~/singularity-containers"
            tooltip: Git repo containing LFS SIF parts (finetune/finetune.*.sif)
            hidden: ${{ inputs.container.source != 'lfs' }}
            ignore: ${{ inputs.container.source != 'lfs' }}

          lfs_branch:
            label: LFS Branch
            type: string
            default: main
            tooltip: Branch to pull from the LFS repo
            hidden: ${{ inputs.container.source != 'lfs' }}
            ignore: ${{ inputs.container.source != 'lfs' }}

          bucket:
            label: Container Bucket
            type: string
            default: ""
            tooltip: Bucket containing finetune.sif container
            hidden: ${{ inputs.container.source != 'pull' }}
            ignore: ${{ inputs.container.source != 'pull' }}

          finetune_path:
            label: Finetune Container Path
            type: string
            default: ~/pw/singularity/finetune.sif
            placeholder: ~/pw/singularity/finetune.sif
            tooltip: Path to finetune.sif container (use existing or build destination)
            hidden: ${{ inputs.container.source == 'pull' }}
            ignore: ${{ inputs.container.source == 'pull' }}

      dataset_config:
        type: group
        label: Dataset Configuration
        collapsed: true
        items:
          dataset_source:
            label: Dataset Source
            type: dropdown
            default: huggingface
            options:
              - value: huggingface
                label: HuggingFace Hub
              - value: local
                label: Local File Path
              - value: bucket
                label: PW Storage Bucket

          dataset_cache_dir:
            label: Dataset Cache Directory
            type: string
            default: "/home/$USER/pw/datasets"
            optional: true
            tooltip: Shared location for caching HuggingFace datasets across runs

          dataset_name:
            label: Dataset Name (HuggingFace)
            type: string
            default: "${{ inputs.model_selection.model_config == 'llama3-8b-medical-quick' ? 'Shekswess/medical_llama3_instruct_dataset_short' : (inputs.model_selection.model_config == 'mistral-7b-medical' ? 'Shekswess/medical_mistral_instruct_dataset' : (inputs.model_selection.model_config == 'gemma-7b-medical' ? 'Shekswess/medical_gemma_instruct_dataset' : 'Shekswess/medical_llama3_instruct_dataset')) }}"
            hidden: ${{ inputs.dataset_config.dataset_source != 'huggingface' }}

          dataset_bucket:
            label: Dataset Storage Bucket
            type: storage
            default: ""
            optional: true
            hidden: ${{ inputs.dataset_config.dataset_source != 'bucket' }}

          dataset_bucket_path:
            label: Dataset Bucket Path
            type: string
            default: ""
            optional: true
            hidden: ${{ inputs.dataset_config.dataset_source != 'bucket' }}

          local_dataset_path:
            label: Local Dataset Path
            type: string
            default: ""
            optional: true
            hidden: ${{ inputs.dataset_config.dataset_source != 'local' }}

          dataset_split:
            label: Dataset Split
            type: string
            default: train
            optional: true
            hidden: ${{ inputs.dataset_config.dataset_source == 'bucket' }}

          dataset_config_name:
            label: Dataset Config Name
            type: string
            default: ""
            optional: true
            hidden: ${{ inputs.dataset_config.dataset_source != 'huggingface' }}

          dataset_format:
            label: Dataset Format
            type: dropdown
            default: json
            optional: true
            hidden: ${{ inputs.dataset_config.dataset_source != 'local' }}
            options:
              - value: json
                label: JSON / JSONL
              - value: csv
                label: CSV
              - value: parquet
                label: Parquet
              - value: arrow
                label: Arrow
              - value: dataset
                label: HuggingFace Dataset Directory

          prompt_field:
            label: Prompt Field Name
            type: string
            default: prompt
            optional: true

          max_samples:
            label: Max Samples (0 = all)
            type: number
            default: "${{ inputs.model_selection.model_config == 'llama3-8b-medical-quick' ? 50 : 0 }}"
            optional: true

      training_hyperparameters:
        type: group
        label: Training Hyperparameters
        collapsed: true
        items:
          num_epochs:
            label: Training Epochs
            type: number
            default: "${{ inputs.model_selection.model_config == 'llama3-8b-medical-quick' ? 1.0 : 3.0 }}"
            optional: true

          learning_rate:
            label: Learning Rate
            type: number
            default: 0.0002
            optional: true

          micro_batch_size:
            label: Micro Batch Size (per GPU)
            type: number
            default: "${{ inputs.model_selection.model_config == 'llama3-8b-medical-quick' ? 2 : 1 }}"
            optional: true

          gradient_accumulation:
            label: Gradient Accumulation Steps
            type: number
            default: "${{ inputs.model_selection.model_config == 'llama3-8b-medical-quick' ? 8 : 16 }}"
            optional: true

          max_seq_length:
            label: Max Sequence Length (tokens)
            type: number
            default: "${{ inputs.model_selection.model_config == 'llama3-8b-medical-quick' ? 512 : 2048 }}"
            optional: true

      lora_config:
        type: group
        label: LoRA / PEFT Configuration
        collapsed: true
        items:
          lora_r:
            label: LoRA Rank (r)
            type: number
            default: "${{ inputs.model_selection.model_config == 'llama3-8b-medical-quick' ? 32 : 64 }}"
            optional: true

          lora_alpha:
            label: LoRA Alpha
            type: number
            default: 16
            optional: true

          lora_dropout:
            label: LoRA Dropout Rate
            type: number
            default: 0.05
            optional: true

          quantization:
            label: Quantization Mode
            type: dropdown
            default: 4bit
            optional: true
            options:
              - value: 4bit
              - value: 8bit
              - value: none

          gradient_checkpointing:
            label: Enable Gradient Checkpointing
            type: boolean
            default: true
            optional: true

          bf16:
            label: Use BF16 Mixed Precision
            type: boolean
            default: true
            optional: true

          packing:
            label: Enable Dataset Packing
            type: boolean
            default: false
            optional: true

      tensorboard_config:
        type: group
        label: TensorBoard Monitoring
        collapsed: true
        items:
          tensorboard_enabled:
            label: Enable TensorBoard
            type: boolean
            default: true
            optional: true

          tensorboard_port:
            label: TensorBoard Port
            type: number
            default: 6006
            optional: true
            hidden: ${{ !inputs.tensorboard_config.tensorboard_enabled }}

      output_publishing:
        type: group
        label: Output & Publishing
        collapsed: true
        items:
          merge_full_weights:
            label: Merge LoRA into Full Weights
            type: boolean
            default: true
            optional: true

          merged_save_format:
            label: Merged Weights Format
            type: dropdown
            default: safetensors
            optional: true
            options:
              - value: safetensors
                label: SafeTensors (.safetensors)
              - value: bin
                label: PyTorch Binary (.bin)

          upload_destination:
            label: Upload Weights To
            type: dropdown
            default: none
            optional: true
            options:
              - value: none
                label: No Upload (keep on cluster)
              - value: pw_bucket
                label: PW Storage Bucket
              - value: huggingface
                label: HuggingFace Hub

          bucket_name:
            label: PW Bucket Name
            type: string
            default: ""
            optional: true
            hidden: ${{ inputs.output_publishing.upload_destination != 'pw_bucket' }}
            tooltip: "Bucket name for upload (e.g., my-bucket). Will upload as pw://bucket-name/medical-finetune-JOBID.tgz"

          hub_model_id:
            label: Hub Model ID (username/model-name)
            type: string
            default: ""
            optional: true
            hidden: ${{ inputs.output_publishing.upload_destination != 'huggingface' }}

          hf_token:
            label: HuggingFace Access Token
            type: password
            default: ${{ org.HF_TOKEN }}
            optional: true
            hidden: ${{ inputs.output_publishing.upload_destination != 'huggingface' }}


jobs:
  prepare_job_directory:
    steps:
      - name: Verify GPU availability
        early-cancel: any-job-failed
        run: |
          echo "Checking GPU availability on ${{ inputs.cluster.name }}..."
          nvidia-smi || { echo "No GPU available"; exit 1; }
          echo "GPU(s) found:"
          nvidia-smi --query-gpu=name,memory.total --format=csv,noheader
        ssh:
          remoteHost: ${{ inputs.cluster.ip }}

      - name: Preparing Run Directory
        early-cancel: any-job-failed
        run: |
          RUNDIR="${{ inputs.infrastructure.rundir }}"
          RUNDIR="${RUNDIR/#\~/$HOME}"  # Expand tilde
          
          mkdir -p $(dirname "${RUNDIR}")
          
          if [ -d "${RUNDIR}/.git" ]; then
            echo "Repository exists, updating..."
            cd "${RUNDIR}"
            git fetch origin
            git checkout ${{ inputs.infrastructure.repository_branch }}
            git branch --set-upstream-to=origin/${{ inputs.infrastructure.repository_branch }} 2>/dev/null || true
            git pull
          else
            echo "Cloning repository..."
            rm -rf "${RUNDIR}"
            git clone -b ${{ inputs.infrastructure.repository_branch }} ${{ inputs.infrastructure.repository }} "${RUNDIR}"
          fi
          
          cd "${RUNDIR}"
          git log -1 --oneline

          # Clean up any stale marker/output files from previous runs
          rm -f jobid SESSION_PORT job.started job.ended run.out HOSTNAME
          
          # Create output directory
          mkdir -p "${HOME}/pw/outputs/medical-finetune-${PW_JOB_NUMBER}"
          
          # Output RUN_ID for downstream jobs
          echo "RUN_ID=${PW_JOB_NUMBER}" >> $OUTPUTS
          echo "RUNDIR=${RUNDIR}" >> $OUTPUTS
          
          echo "======================================"
          echo "Job Configuration"
          echo "======================================"
          echo "PW_JOB_NUMBER: ${PW_JOB_NUMBER}"
          echo "RUNDIR: ${RUNDIR}"
          echo "OUTPUT_DIR: ${HOME}/pw/outputs/medical-finetune-${PW_JOB_NUMBER}"
          echo "======================================"
        ssh:
          remoteHost: ${{ inputs.cluster.ip }}

  prepare-model:
    needs: [prepare_job_directory]
    if: ${{ inputs.model_selection.model_source == 'huggingface' }}
    steps:
      - name: Clone HuggingFace model
        early-cancel: any-job-failed
        run: |
          set -euo pipefail
          cd "${{ needs.prepare_job_directory.outputs.RUNDIR }}"

          command -v git >/dev/null 2>&1 || { echo "ERROR: git not found"; exit 1; }

          MODEL_ID="${{ inputs.model_selection.base_model_id }}"
          CACHE_DIR="${{ inputs.model_selection.model_cache_dir }}"
          CACHE_DIR="${CACHE_DIR/#\~/$HOME}"
          HF_TOKEN="${{ inputs.model_selection.hf_token }}"

          mkdir -p "${CACHE_DIR}"
          MODEL_BASENAME="${MODEL_ID##*/}"
          LEGACY_DIR="${CACHE_DIR}/${MODEL_ID//\//__}"
          TARGET_DIR="${CACHE_DIR}/${MODEL_BASENAME}"

          if [[ -d "${LEGACY_DIR}" && ! -d "${TARGET_DIR}" ]]; then
            echo "Renaming legacy cache dir ${LEGACY_DIR} to ${TARGET_DIR}"
            mv "${LEGACY_DIR}" "${TARGET_DIR}"
          fi

          if [[ -d "${TARGET_DIR}" && -f "${TARGET_DIR}/config.json" ]]; then
            echo "Model already cached at ${TARGET_DIR}"
          else
            echo "Cloning model ${MODEL_ID} to ${TARGET_DIR}"

            # Ensure git-lfs is available
            if ! git lfs version >/dev/null 2>&1; then
              echo "Installing git-lfs locally..."
              mkdir -p "${HOME}/bin"
              cd /tmp
              if command -v curl >/dev/null 2>&1; then
                LFS_URL=$(curl -s https://api.github.com/repos/git-lfs/git-lfs/releases/latest | grep browser_download_url | grep linux-amd64 | head -1 | cut -d '"' -f 4)
              elif command -v wget >/dev/null 2>&1; then
                LFS_URL=$(wget -qO- https://api.github.com/repos/git-lfs/git-lfs/releases/latest | grep browser_download_url | grep linux-amd64 | head -1 | cut -d '"' -f 4)
              else
                echo "ERROR: curl or wget required to install git-lfs"
                exit 1
              fi
              [[ -z "$LFS_URL" ]] && { echo "ERROR: Could not find git-lfs download URL"; exit 1; }
              if command -v wget >/dev/null 2>&1; then
                wget -q "$LFS_URL" -O git-lfs-linux-amd64.tar.gz
              else
                curl -L "$LFS_URL" -o git-lfs-linux-amd64.tar.gz
              fi
              tar -xzf git-lfs-linux-amd64.tar.gz
              ./git-lfs-*/install.sh --local
              rm -rf git-lfs-*
              export PATH="${HOME}/bin:${PATH}"
              cd "${{ needs.prepare_job_directory.outputs.RUNDIR }}"
              git lfs version >/dev/null 2>&1 || { echo "ERROR: Failed to install git-lfs"; exit 1; }
            fi

            git lfs install

            REPO_URL="https://huggingface.co/${MODEL_ID}"
            [[ -n "${HF_TOKEN}" ]] && REPO_URL="https://user:${HF_TOKEN}@huggingface.co/${MODEL_ID}"

            GIT_LFS_SKIP_SMUDGE=1 git clone --depth 1 "${REPO_URL}" "${TARGET_DIR}"
            cd "${TARGET_DIR}"
            git lfs pull
            cd ..

            # Verify model weights exist (not just LFS pointers)
            if [[ -f "${TARGET_DIR}/model.safetensors" ]]; then
              actual_size=$(stat -c%s "${TARGET_DIR}/model.safetensors" 2>/dev/null || stat -f%z "${TARGET_DIR}/model.safetensors" 2>/dev/null)
              [[ ${actual_size} -lt 1000000 ]] && { echo "ERROR: model.safetensors appears to be an LFS pointer, not actual weights"; exit 1; }
            elif ls "${TARGET_DIR}"/model*.safetensors 1>/dev/null 2>&1; then
              for f in "${TARGET_DIR}"/model*.safetensors; do
                actual_size=$(stat -c%s "${f}" 2>/dev/null || stat -f%z "${f}" 2>/dev/null)
                [[ ${actual_size} -lt 1000000 ]] && { echo "ERROR: ${f} appears to be an LFS pointer, not actual weights"; exit 1; }
                break
              done
            fi

            [[ ! -f "${TARGET_DIR}/config.json" ]] && { echo "ERROR: Model clone incomplete - missing config.json"; exit 1; }
          fi

          echo "MODEL_PATH=${TARGET_DIR}" >> $OUTPUTS
          echo "Model ready at ${TARGET_DIR}"
        ssh:
          remoteHost: ${{ inputs.cluster.ip }}

  prepare-dataset:
    needs: [prepare_job_directory]
    steps:
      - name: Prepare dataset based on source
        early-cancel: any-job-failed
        run: |
          OUTPUT_DIR="${HOME}/pw/outputs/medical-finetune-${{ needs.prepare_job_directory.outputs.RUN_ID }}"
          DATASET_DIR="${OUTPUT_DIR}/dataset"
          DATASET_CACHE_DIR="${{ inputs.dataset_config.dataset_cache_dir }}"
          mkdir -p "${DATASET_DIR}" "${DATASET_CACHE_DIR}"

          DATASET_SOURCE="${{ inputs.dataset_config.dataset_source }}"
          DATASET_NAME="${{ inputs.dataset_config.dataset_name }}"
          echo "Dataset source: ${DATASET_SOURCE}"

          if [ "${DATASET_SOURCE}" == "bucket" ]; then
            echo "Downloading dataset from PW Storage Bucket..."
            BUCKET="${{ inputs.dataset_config.dataset_bucket }}"
            BUCKET_PATH="${{ inputs.dataset_config.dataset_bucket_path }}"

            if [ -z "${BUCKET}" ] || [ -z "${BUCKET_PATH}" ]; then
              echo "Error: bucket and bucket_path must be specified for bucket source"
              exit 1
            fi

            pw storage cp -r "${BUCKET}/${BUCKET_PATH}" "${DATASET_DIR}/"
            echo "Dataset downloaded to ${DATASET_DIR}"
            ls -la "${DATASET_DIR}"

          elif [ "${DATASET_SOURCE}" == "local" ]; then
            echo "Using local dataset path..."
            LOCAL_PATH="${{ inputs.dataset_config.local_dataset_path }}"

            if [ ! -e "${LOCAL_PATH}" ]; then
              echo "Error: Local dataset path not found: ${LOCAL_PATH}"
              exit 1
            fi

            if [ -f "${LOCAL_PATH}" ]; then
              cp "${LOCAL_PATH}" "${DATASET_DIR}/"
            elif [ -d "${LOCAL_PATH}" ]; then
              cp -r "${LOCAL_PATH}"/* "${DATASET_DIR}/"
            fi
            echo "Dataset copied to ${DATASET_DIR}"
            ls -la "${DATASET_DIR}"

          else
            echo "Using HuggingFace Hub dataset: ${DATASET_NAME}"
            echo "Cache directory: ${DATASET_CACHE_DIR}"
            
            # Check if dataset is already cached
            # HuggingFace datasets library will use HF_DATASETS_CACHE
            export HF_DATASETS_CACHE="${DATASET_CACHE_DIR}"
            
            # Create a marker file to track cached datasets
            CACHE_MARKER="${DATASET_CACHE_DIR}/.cached_datasets"
            if grep -q "^${DATASET_NAME}$" "${CACHE_MARKER}" 2>/dev/null; then
              echo "Dataset '${DATASET_NAME}' found in cache!"
            else
              echo "Dataset will be downloaded on first use and cached for future runs"
            fi
          fi
        ssh:
          remoteHost: ${{ inputs.cluster.ip }}

  build-container:
    needs: [prepare-dataset]
    steps:
      - name: Prepare Singularity container
        early-cancel: any-job-failed
        run: |
          set -euo pipefail
          RUNDIR="${{ inputs.infrastructure.rundir }}"
          RUNDIR="${RUNDIR/#\~/$HOME}"  # Expand tilde

          CONTAINER_SOURCE="${{ inputs.container.source }}"
          FINETUNE_PATH="${{ inputs.container.finetune_path }}"
          FINETUNE_PATH="${FINETUNE_PATH/#\~/$HOME}"
          if [ -z "${FINETUNE_PATH}" ]; then
            FINETUNE_PATH="${HOME}/pw/singularity/finetune.sif"
          fi

          install_git_lfs() {
            if git lfs version >/dev/null 2>&1; then
              return 0
            fi
            echo "git-lfs not found; attempting install..."
            if command -v apt-get >/dev/null 2>&1; then
              if sudo -n true 2>/dev/null; then
                sudo apt-get update -y && sudo apt-get install -y git-lfs
              else
                apt-get update -y && apt-get install -y git-lfs
              fi
            elif command -v dnf >/dev/null 2>&1; then
              if sudo -n true 2>/dev/null; then
                sudo dnf install -y git-lfs
              else
                dnf install -y git-lfs
              fi
            elif command -v yum >/dev/null 2>&1; then
              if sudo -n true 2>/dev/null; then
                sudo yum install -y git-lfs
              else
                yum install -y git-lfs
              fi
            elif command -v microdnf >/dev/null 2>&1; then
              if sudo -n true 2>/dev/null; then
                sudo microdnf install -y git-lfs
              else
                microdnf install -y git-lfs
              fi
            elif command -v zypper >/dev/null 2>&1; then
              if sudo -n true 2>/dev/null; then
                sudo zypper --non-interactive install git-lfs
              else
                zypper --non-interactive install git-lfs
              fi
            else
              echo "ERROR: git-lfs not found and no supported package manager available."
              exit 1
            fi
            git lfs version >/dev/null 2>&1 || { echo "ERROR: git-lfs install failed"; exit 1; }
          }

          case "${CONTAINER_SOURCE}" in
            path)
              if [ ! -f "${FINETUNE_PATH}" ]; then
                echo "Error: Image not found at ${FINETUNE_PATH}"
                exit 1
              fi
              SIF_PATH="${FINETUNE_PATH}"
              ;;
            pull)
              BUCKET="${{ inputs.container.bucket }}"
              if [ -z "${BUCKET}" ]; then
                echo "Error: container bucket must be specified for pull source"
                exit 1
              fi
              SIF_PATH="${RUNDIR}/finetune.sif"
              if [ ! -f "${SIF_PATH}" ]; then
                pw bucket cp "${BUCKET}/finetune.sif" "${SIF_PATH}"
              else
                echo "Using existing container: ${SIF_PATH}"
              fi
              ;;
            lfs)
              command -v git >/dev/null 2>&1 || { echo "ERROR: git not found"; exit 1; }
              install_git_lfs
              export GIT_LFS_SKIP_SMUDGE=1

              LFS_REPO="${{ inputs.container.lfs_repo }}"
              LFS_REPO="${LFS_REPO/#\~/$HOME}"
              LFS_BRANCH="${{ inputs.container.lfs_branch }}"
              LFS_DIR="${RUNDIR}/.lfs_containers"
              REV_FILE="${RUNDIR}/.lfs_containers_rev"

              if [ -z "${LFS_REPO}" ]; then
                echo "ERROR: LFS repo must be specified"
                exit 1
              fi

              need_finetune=true
              [[ -f "${FINETUNE_PATH}" ]] && need_finetune=false

              remote_rev=$(git ls-remote "${LFS_REPO}" "${LFS_BRANCH}" | awk '{print $1}')
              if [[ -z "${remote_rev}" ]]; then
                echo "ERROR: Could not resolve ${LFS_BRANCH} in ${LFS_REPO}"
                exit 1
              fi
              stored_rev=""
              [[ -f "${REV_FILE}" ]] && stored_rev=$(cat "${REV_FILE}" || true)

              if [[ "${need_finetune}" == "false" ]]; then
                if [[ -n "${remote_rev}" && "${remote_rev}" == "${stored_rev}" ]]; then
                  echo "Container already present and matches remote revision ${remote_rev}; skipping LFS pull."
                else
                  echo "Container already present; remote revision is ${remote_rev} (stored: ${stored_rev})."
                  echo "Skipping LFS pull to avoid re-download. Remove container to force refresh."
                fi
                SIF_PATH="${FINETUNE_PATH}"
              else
                if [[ -d "${LFS_DIR}/.git" ]]; then
                  git -C "${LFS_DIR}" fetch origin "${LFS_BRANCH}"
                else
                  GIT_LFS_SKIP_SMUDGE=1 git clone --depth 1 --branch "${LFS_BRANCH}" "${LFS_REPO}" "${LFS_DIR}"
                fi

                git -C "${LFS_DIR}" checkout -f "${LFS_BRANCH}"
                git -C "${LFS_DIR}" reset --hard "origin/${LFS_BRANCH}"
                git -C "${LFS_DIR}" lfs install --local

                git -C "${LFS_DIR}" sparse-checkout init --no-cone
                patterns=("finetune/finetune.*.sif" "finetune/finetune.sif.part_*" "finetune/finetune.sif" "scripts/sif_parts.sh")
                git -C "${LFS_DIR}" sparse-checkout set --no-cone "${patterns[@]}"

                lfs_include="finetune/finetune.*.sif,finetune/finetune.sif.part_*,finetune/finetune.sif"
                git -C "${LFS_DIR}" config lfs.concurrenttransfers 4
                git -C "${LFS_DIR}" config lfs.transfer.maxretries 10
                git -C "${LFS_DIR}" config lfs.transfer.maxretrydelay 30
                git -C "${LFS_DIR}" config lfs.fetchinclude "${lfs_include}"
                git -C "${LFS_DIR}" config lfs.fetchexclude ""
                attempt=1
                max_attempts=2
                while true; do
                  echo "LFS pull attempt ${attempt}/${max_attempts}..."
                  if git -C "${LFS_DIR}" lfs fetch; then
                    break
                  fi
                  if [[ ${attempt} -ge ${max_attempts} ]]; then
                    echo "ERROR: LFS pull failed after ${max_attempts} attempts"
                    exit 1
                  fi
                  sleep $((attempt * 5))
                  ((attempt++))
                done
                git -C "${LFS_DIR}" lfs checkout

                assemble_sif() {
                  local prefix=$1
                  local target=$2
                  local full="${LFS_DIR}/${prefix}/${prefix}.sif"
                  mkdir -p "$(dirname "${target}")"

                  if [[ -f "${full}" ]]; then
                    cp "${full}" "${target}"
                    return 0
                  fi

                  local script="${LFS_DIR}/scripts/sif_parts.sh"
                  shopt -s nullglob
                  local numeric_parts=("${LFS_DIR}/${prefix}/${prefix}."[0-9][0-9][0-9][0-9][0-9].sif)
                  shopt -u nullglob
                  if [[ -x "${script}" && ${#numeric_parts[@]} -gt 0 ]]; then
                    bash "${script}" join --prefix "${prefix}" --in-dir "${LFS_DIR}" --output "${target}"
                    return 0
                  fi

                  shopt -s nullglob
                  local parts=("${LFS_DIR}/${prefix}/${prefix}."[0-9][0-9][0-9][0-9][0-9].sif "${LFS_DIR}/${prefix}/${prefix}.sif.part_"*)
                  shopt -u nullglob

                  if ((${#parts[@]} == 0)); then
                    echo "ERROR: No parts found for ${prefix} in ${LFS_DIR}/${prefix}"
                    exit 1
                  fi

                  mapfile -t parts_sorted < <(printf "%s\n" "${parts[@]}" | LC_ALL=C sort)
                  cat "${parts_sorted[@]}" > "${target}"
                }

                echo "Assembling finetune container to ${FINETUNE_PATH}"
                assemble_sif finetune "${FINETUNE_PATH}"
                [[ -n "${remote_rev}" ]] && echo "${remote_rev}" > "${REV_FILE}"
                SIF_PATH="${FINETUNE_PATH}"
              fi
              ;;
            build)
              DEF_FILE="${RUNDIR}/singularity/finetune.def"
              SIF_PATH="${FINETUNE_PATH}"
              mkdir -p "$(dirname "${SIF_PATH}")"

              if [ ! -f "${SIF_PATH}" ]; then
                echo "Building Singularity image from ${DEF_FILE}..."
                if sudo -n true 2>/dev/null; then
                  sudo singularity build "${SIF_PATH}" "${DEF_FILE}"
                else
                  singularity build --fakeroot "${SIF_PATH}" "${DEF_FILE}" || \
                    singularity build "${SIF_PATH}" "${DEF_FILE}"
                fi
              else
                echo "Using existing Singularity image: ${SIF_PATH}"
              fi
              ;;
            *)
              echo "Error: Unknown container source ${CONTAINER_SOURCE}"
              exit 1
              ;;
          esac

          echo "Container ready: ${SIF_PATH}"
          echo "SIF_PATH=${SIF_PATH}" >> $OUTPUTS
        ssh:
          remoteHost: ${{ inputs.cluster.ip }}
        env:
          SINGULARITY_DISABLE_CACHE: "false"

  finetune:
    needs: [build-container, prepare-dataset, prepare_job_directory, prepare-model]
    working-directory: ${{ needs.prepare_job_directory.outputs.RUNDIR }}
    ssh:
      remoteHost: ${{ inputs.cluster.ip }}
    steps:
      - name: Create fine-tuning script
        early-cancel: any-job-failed
        run: |
          SCRIPT_PATH="${{ needs.prepare_job_directory.outputs.RUNDIR }}/start_finetune.sh"
          cat > "${SCRIPT_PATH}" << 'EOF'
          #!/bin/bash
          OUTPUT_DIR="${HOME}/pw/outputs/medical-finetune-${{ needs.prepare_job_directory.outputs.RUN_ID }}"
          WORKDIR="${{ needs.prepare_job_directory.outputs.RUNDIR }}"

          # Get SIF path from build-container job output
          export SIF_PATH="${{ needs.build-container.outputs.SIF_PATH }}"

          # Fallback if output not available
          if [ -z "${SIF_PATH}" ] || [ ! -f "${SIF_PATH}" ]; then
            FINETUNE_PATH="${{ inputs.container.finetune_path }}"
            FINETUNE_PATH="${FINETUNE_PATH/#\~/$HOME}"

            if [ "${{ inputs.container.source }}" == "pull" ] && [ -f "${WORKDIR}/finetune.sif" ]; then
              export SIF_PATH="${WORKDIR}/finetune.sif"
            elif [ -n "${FINETUNE_PATH}" ] && [ -f "${FINETUNE_PATH}" ]; then
              export SIF_PATH="${FINETUNE_PATH}"
            elif [ -f "${HOME}/pw/singularity/finetune.sif" ]; then
              export SIF_PATH="${HOME}/pw/singularity/finetune.sif"
            else
              echo "Error: Singularity image not found"
              exit 1
            fi
          fi

          echo "Using Singularity image: ${SIF_PATH}"

          # Export all training configuration as environment variables
          export OUTPUT_DIR="${OUTPUT_DIR}"

          # Set MODEL_ID based on source (HuggingFace or local path)
          if [ "${{ inputs.model_selection.model_source }}" == "local" ]; then
            export MODEL_ID="${{ inputs.model_selection.local_model_path }}"
            echo "Using local model weights: ${MODEL_ID}"
          else
            CLONED_MODEL="${{ needs.prepare-model.outputs.MODEL_PATH }}"
            if [ -n "${CLONED_MODEL}" ] && [ -d "${CLONED_MODEL}" ]; then
              export MODEL_ID="${CLONED_MODEL}"
              echo "Using cached HuggingFace model: ${MODEL_ID}"
            else
              export MODEL_ID="${{ inputs.model_selection.base_model_id }}"
              echo "Using HuggingFace model: ${MODEL_ID}"
            fi
          fi

          export DATASET_SOURCE="${{ inputs.dataset_config.dataset_source }}"
          export DATASET_NAME="${{ inputs.dataset_config.dataset_name }}"
          export DATASET_SPLIT="${{ inputs.dataset_config.dataset_split }}"
          export DATASET_CONFIG_NAME="${{ inputs.dataset_config.dataset_config_name }}"
          export PROMPT_FIELD="${{ inputs.dataset_config.prompt_field }}"
          export LOCAL_DATASET_PATH="${{ inputs.dataset_config.local_dataset_path }}"
          export DATASET_FORMAT="${{ inputs.dataset_config.dataset_format }}"
          export MAX_SAMPLES="${{ inputs.dataset_config.max_samples }}"
          export DATASET_CACHE_DIR="${{ inputs.dataset_config.dataset_cache_dir }}"
          export HF_DATASETS_CACHE="${{ inputs.dataset_config.dataset_cache_dir }}"

          # Training hyperparameters
          export NUM_EPOCHS="${{ inputs.training_hyperparameters.num_epochs }}"
          export LEARNING_RATE="${{ inputs.training_hyperparameters.learning_rate }}"
          export MICRO_BATCH_SIZE="${{ inputs.training_hyperparameters.micro_batch_size }}"
          export GRADIENT_ACCUMULATION="${{ inputs.training_hyperparameters.gradient_accumulation }}"
          export MAX_SEQ_LENGTH="${{ inputs.training_hyperparameters.max_seq_length }}"

          # LoRA configuration
          export LORA_R="${{ inputs.lora_config.lora_r }}"
          export LORA_ALPHA="${{ inputs.lora_config.lora_alpha }}"
          export LORA_DROPOUT="${{ inputs.lora_config.lora_dropout }}"
          export QUANTIZATION="${{ inputs.lora_config.quantization }}"
          export BF16="${{ inputs.lora_config.bf16 }}"
          export GRADIENT_CHECKPOINTING="${{ inputs.lora_config.gradient_checkpointing }}"
          export PACKING="${{ inputs.lora_config.packing }}"

          # Output configuration
          export MERGE_FULL_WEIGHTS="${{ inputs.output_publishing.merge_full_weights }}"
          export MERGED_SAVE_FORMAT="${{ inputs.output_publishing.merged_save_format }}"

          # TensorBoard configuration (auto-configured for PW session proxy)
          export TENSORBOARD_ENABLED="${{ inputs.tensorboard_config.tensorboard_enabled }}"
          export TENSORBOARD_PORT="${{ inputs.tensorboard_config.tensorboard_port }}"
          export TENSORBOARD_PATH_PREFIX="/me/session/${PW_USER}/${{ sessions.session }}"
          export TENSORBOARD_BIND_ALL="true"

          # Debug: Log TensorBoard configuration
          echo "====================================="
          echo "TensorBoard Debug Info"
          echo "====================================="
          echo "PW_USER: ${PW_USER}"
          echo "SESSION_NAME: ${{ sessions.session }}"
          echo "TENSORBOARD_PATH_PREFIX: ${TENSORBOARD_PATH_PREFIX}"
          echo "TENSORBOARD_PORT: ${TENSORBOARD_PORT}"
          echo "TENSORBOARD_ENABLED: ${TENSORBOARD_ENABLED}"
          echo "====================================="

          # Restrict to single GPU for QLoRA training (avoids DataParallel issues)
          export CUDA_VISIBLE_DEVICES="0"

          # HuggingFace Hub configuration
          if [ -n "${{ inputs.output_publishing.hf_token }}" ]; then
            export HF_TOKEN="${{ inputs.output_publishing.hf_token }}"
          fi

          # Handle bucket dataset source
          if [ "${{ inputs.dataset_config.dataset_source }}" == "bucket" ]; then
            export DATASET_DIR="${OUTPUT_DIR}/dataset"
          fi

          echo "======================================"
          echo "Training Configuration"
          echo "======================================"
          echo "Model: ${MODEL_ID}"
          echo "Dataset Source: ${DATASET_SOURCE}"
          echo "Dataset: ${DATASET_NAME:-${LOCAL_DATASET_PATH:-bucket}}"
          echo "Epochs: ${NUM_EPOCHS}"
          echo "Max Samples: ${MAX_SAMPLES}"
          echo "TensorBoard: ${TENSORBOARD_ENABLED}"
          echo "======================================"

          # Execute training using run.sh
          cd "${WORKDIR}"
          ./scripts/run.sh

          echo "Training completed. Checking outputs..."
          ls -la "${OUTPUT_DIR}/adapters" 2>/dev/null || echo "No adapters found"
          if [ "${MERGE_FULL_WEIGHTS}" == "true" ]; then
            ls -la "${OUTPUT_DIR}/merged" 2>/dev/null || echo "No merged weights found"
          fi
          EOF
          chmod +x "${SCRIPT_PATH}"

      - name: Run fine-tuning
        uses: marketplace/job_runner/v4.0
        early-cancel: any-job-failed
        with:
          resource: ${{ inputs.cluster }}
          shebang: '#!/bin/bash'
          rundir: ${{ needs.prepare_job_directory.outputs.RUNDIR }}
          use_existing_script: true
          script_path: ${{ needs.prepare_job_directory.outputs.RUNDIR }}/start_finetune.sh
          scheduler: ${{ inputs.submit_to_scheduler }}
          inject_markers: true
          slurm:
            is_disabled: ${{ inputs.submit_to_scheduler != true || inputs.cluster.schedulerType != 'slurm' }}
            account: ${{ inputs.slurm.account }}
            partition: ${{ inputs.slurm.partition }}
            qos: ${{ inputs.slurm.qos }}
            time: ${{ inputs.slurm.time }}
            cpus_per_task: ${{ inputs.slurm.cpus_per_task }}
            gres: ${{ inputs.slurm.gres }}
            scheduler_directives: |
              ${{ inputs.slurm.scheduler_directives }}
          pbs:
            is_disabled: ${{ inputs.submit_to_scheduler != true || inputs.cluster.schedulerType != 'pbs' }}
            account: ${{ inputs.pbs.account }}
            queue: ${{ inputs.pbs.queue }}
            walltime: ${{ inputs.pbs.walltime }}
            select: ${{ inputs.pbs.select }}
            scheduler_directives: |
              ${{ inputs.pbs.scheduler_directives }}

      - name: Validate outputs
        early-cancel: any-job-failed
        run: |
          OUTPUT_DIR="${HOME}/pw/outputs/medical-finetune-${{ needs.prepare_job_directory.outputs.RUN_ID }}"
          
          echo "======================================"
          echo "Output Validation Debug"
          echo "======================================"
          echo "OUTPUT_DIR: ${OUTPUT_DIR}"
          echo "RUN_ID: ${{ needs.prepare_job_directory.outputs.RUN_ID }}"
          echo ""
          echo "Contents of OUTPUT_DIR:"
          ls -la "${OUTPUT_DIR}" 2>/dev/null || echo "OUTPUT_DIR does not exist"
          echo ""
          echo "Looking for adapters directory..."
          find "${OUTPUT_DIR}" -type d -name "adapters" 2>/dev/null || echo "No adapters directory found"
          echo ""
          echo "All directories in OUTPUT_DIR:"
          find "${OUTPUT_DIR}" -type d 2>/dev/null | head -20 || echo "No directories found"
          echo "======================================"
          
          if [ ! -d "${OUTPUT_DIR}/adapters" ]; then
            echo "Error: No adapter outputs found at ${OUTPUT_DIR}/adapters"
            echo "Checking if adapters exist elsewhere..."
            find "${HOME}/pw/outputs" -type d -name "adapters" 2>/dev/null | head -5
            exit 1
          fi

          if [ "${{ inputs.output_publishing.merge_full_weights }}" == "true" ] && [ ! -d "${OUTPUT_DIR}/merged" ]; then
            echo "Error: Merge requested but no merged weights found"
            exit 1
          fi

          echo "Training outputs validated successfully"
          echo "Adapter files:"
          ls -la "${OUTPUT_DIR}/adapters"

          if [ -d "${OUTPUT_DIR}/merged" ]; then
            echo "Merged weight files:"
            ls -la "${OUTPUT_DIR}/merged"
          fi

  tensorboard-session:
    needs: [build-container, prepare-dataset]
    if: ${{ inputs.tensorboard_config.tensorboard_enabled }}
    steps:
      - name: Wait for TensorBoard to start
        early-cancel: any-job-failed
        run: |
          echo "Waiting for TensorBoard to be available on port ${{ inputs.tensorboard_config.tensorboard_port }}..."
          # Wait up to 5 minutes for TensorBoard to start (finetune job starts it)
          for i in {1..150}; do
            if nc -z localhost ${{ inputs.tensorboard_config.tensorboard_port }} 2>/dev/null; then
              echo "TensorBoard is running on port ${{ inputs.tensorboard_config.tensorboard_port }}!"
              exit 0
            fi
            sleep 2
          done
          echo "Warning: TensorBoard did not start within timeout"
          exit 1
        ssh:
          remoteHost: ${{ inputs.cluster.ip }}
          
      - name: Update TensorBoard Session
        uses: parallelworks/update-session
        with:
          remotePort: ${{ inputs.tensorboard_config.tensorboard_port }}
          name: ${{ sessions.session }}
          status: running
          target: ${{ inputs.cluster.id }}
          slug: ""
          
      - name: Keep session alive during training
        run: |
          echo "TensorBoard session is active. Waiting for training to complete..."
          # Keep session job running - it will be terminated when workflow completes
          while nc -z localhost ${{ inputs.tensorboard_config.tensorboard_port }} 2>/dev/null; do
            sleep 30
          done
          echo "TensorBoard stopped - training likely completed"
        ssh:
          remoteHost: ${{ inputs.cluster.ip }}

  upload-outputs:
    needs: [finetune, prepare_job_directory]
    if: ${{ inputs.output_publishing.upload_destination != 'none' }}
    steps:
      - name: Upload weights to PW bucket
        if: ${{ inputs.output_publishing.upload_destination == 'pw_bucket' }}
        early-cancel: any-job-failed
        run: |
          OUTPUT_DIR="${HOME}/pw/outputs/medical-finetune-${{ needs.prepare_job_directory.outputs.RUN_ID }}"
          BUCKET_NAME="${{ inputs.output_publishing.bucket_name }}"
          ARCHIVE_NAME="medical-finetune-${{ needs.prepare_job_directory.outputs.RUN_ID }}.tgz"

          if [ -z "${BUCKET_NAME}" ]; then
            echo "Error: Bucket name is required for PW bucket upload"
            exit 1
          fi

          echo "Creating archive of outputs..."
          cd "${OUTPUT_DIR}"
          tar -czvf "/tmp/${ARCHIVE_NAME}" .

          echo "Uploading to pw://${BUCKET_NAME}/${ARCHIVE_NAME}..."
          pw bucket cp "/tmp/${ARCHIVE_NAME}" "pw://${BUCKET_NAME}/${ARCHIVE_NAME}"

          echo "Upload complete!"
          echo "Download with: pw bucket cp pw://${BUCKET_NAME}/${ARCHIVE_NAME} ."
          
          # Cleanup temp file
          rm -f "/tmp/${ARCHIVE_NAME}"
        ssh:
          remoteHost: ${{ inputs.cluster.ip }}

      - name: Upload weights to HuggingFace Hub
        if: ${{ inputs.output_publishing.upload_destination == 'huggingface' }}
        early-cancel: any-job-failed
        run: |
          OUTPUT_DIR="${HOME}/pw/outputs/medical-finetune-${{ needs.prepare_job_directory.outputs.RUN_ID }}"
          HUB_MODEL_ID="${{ inputs.output_publishing.hub_model_id }}"
          
          if [ -z "${HUB_MODEL_ID}" ]; then
            echo "Error: Hub Model ID is required for HuggingFace upload"
            exit 1
          fi

          echo "Uploading to HuggingFace Hub: ${HUB_MODEL_ID}..."
          
          # Upload merged weights if available, otherwise adapters
          if [ -d "${OUTPUT_DIR}/merged" ]; then
            echo "Uploading merged weights..."
            huggingface-cli upload "${HUB_MODEL_ID}" "${OUTPUT_DIR}/merged" --token "${HF_TOKEN}"
          elif [ -d "${OUTPUT_DIR}/adapters" ]; then
            echo "Uploading adapter weights..."
            huggingface-cli upload "${HUB_MODEL_ID}" "${OUTPUT_DIR}/adapters" --token "${HF_TOKEN}"
          else
            echo "Error: No weights found to upload"
            exit 1
          fi

          echo "Upload complete!"
          echo "View at: https://huggingface.co/${HUB_MODEL_ID}"
        ssh:
          remoteHost: ${{ inputs.cluster.ip }}
        env:
          HF_TOKEN: ${{ inputs.output_publishing.hf_token }}

  cleanup:
    needs: [finetune, prepare_job_directory]
    if: ${{ always }}
    steps:
      - name: Display training summary
        early-cancel: any-job-failed
        run: |
          OUTPUT_DIR="${HOME}/pw/outputs/medical-finetune-${{ needs.prepare_job_directory.outputs.RUN_ID }}"
          echo "======================================"
          echo "Medical LLM Fine-tuning Summary"
          echo "======================================"
          echo "Run ID: ${{ needs.prepare_job_directory.outputs.RUN_ID }}"
          echo "Model: ${{ inputs.model_selection.base_model_id }}"
          echo "Dataset: ${{ inputs.dataset_config.dataset_name }}"
          echo "Output Directory: ${OUTPUT_DIR}"
          echo ""
          if [ -f "${OUTPUT_DIR}/logs/finetune.log" ]; then
            echo "Last 20 lines of training log:"
            tail -20 "${OUTPUT_DIR}/logs/finetune.log"
          fi
          echo "======================================"
        ssh:
          remoteHost: ${{ inputs.cluster.ip }}

# Note: Configuration presets are now implemented via dynamic defaults in the form inputs.
# When user selects a preset from the dropdown, all related fields automatically update.
# Users can still override any value after selecting a preset.
